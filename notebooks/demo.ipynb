{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airline Tweet Sentiment Analysis - Final Results Demo\n",
    "\n",
    "## üéØ Production-Ready Sentiment Analysis System\n",
    "\n",
    "**Final Performance**: **74.38% weighted F1-score** using SVM with RBF kernel\n",
    "\n",
    "This notebook demonstrates the completed sentiment analysis system that achieved excellent performance through systematic experimentation.\n",
    "\n",
    "### üèÜ Key Achievements\n",
    "- **Best Model**: SVM with RBF kernel  \n",
    "- **Performance**: 74.38% F1-score, 73.57% accuracy\n",
    "- **Methodology**: Systematic experimentation with 10+ configurations\n",
    "- **Engineering**: Production-ready modular architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and setup\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('..')\n",
    "sys.path.append('../src')\n",
    "\n",
    "print(\"üìö Libraries imported successfully!\")\n",
    "print(\"üéØ Ready to demonstrate the completed system\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Systematic Experiment Results\n",
    "\n",
    "The breakthrough was achieved through systematic experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load experimental results\n",
    "with open('../experiments/results/experiment_comparison.json', 'r') as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "print(\"üß™ SYSTEMATIC EXPERIMENT RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Show top experiments\n",
    "experiments_df = pd.DataFrame(results['summary'])\n",
    "experiments_df = experiments_df.sort_values('test_f1', ascending=False)\n",
    "\n",
    "print(f\"{'Rank':<4} {'Experiment':<25} {'F1-Score':<10} {'Model':<15}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for i, (_, row) in enumerate(experiments_df.head().iterrows(), 1):\n",
    "    print(f\"{i:<4} {row['experiment_id']:<25} {row['test_f1']:<10.4f} {row['model_type']:<15}\")\n",
    "\n",
    "best = results['best_experiment']\n",
    "print(f\"\\nüèÜ WINNER: {best['experiment_id']}\")\n",
    "print(f\"   F1-Score: {best['test_f1']:.4f}\")\n",
    "print(f\"   Model: {best['model_type']} with RBF kernel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Final Model Performance\n",
    "\n",
    "Complete evaluation of the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load final evaluation results\n",
    "try:\n",
    "    with open('../final_evaluation/evaluation_report.json', 'r') as f:\n",
    "        final_results = json.load(f)\n",
    "    \n",
    "    print(\"üìã FINAL EVALUATION SUMMARY\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    metrics = final_results['evaluation_metrics']\n",
    "    print(f\"Overall Performance:\")\n",
    "    print(f\"  ‚Ä¢ Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Weighted F1: {metrics['weighted_avg']['f1-score']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nPer-Class Performance:\")\n",
    "    for class_name, class_metrics in metrics['per_class'].items():\n",
    "        print(f\"  ‚Ä¢ {class_name.capitalize()}: F1={class_metrics['f1-score']:.4f}\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö†Ô∏è Run complete_evaluation.py to generate final results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Live Model Demo\n",
    "\n",
    "Test the production model on new tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load production components\n",
    "from src.embeddings import GloVeEmbeddings\n",
    "from src.data_processing import TweetPreprocessor, TweetVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"üîÑ Loading production model...\")\n",
    "\n",
    "# Setup pipeline\n",
    "glove = GloVeEmbeddings('../embeddings/glove.6B.100d.txt')\n",
    "glove.load_embeddings()\n",
    "preprocessor = TweetPreprocessor()\n",
    "vectorizer = TweetVectorizer(glove, preprocessor, aggregation_method='mean')\n",
    "\n",
    "print(\"‚úÖ Production model ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo tweets\n",
    "test_tweets = [\n",
    "    \"Amazing flight! Crew was fantastic and everything on time üòä\",\n",
    "    \"Delayed 3 hours with no explanation. Terrible service.\",\n",
    "    \"Flight was okay, nothing special to report.\",\n",
    "    \"Lost luggage again! This is so frustrating.\",\n",
    "    \"Thank you for the upgrade! Made my trip comfortable.\"\n",
    "]\n",
    "\n",
    "print(\"üß™ LIVE DEMO - Testing Real Tweets\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Quick setup for demo\n",
    "try:\n",
    "    from src.data_processing import load_tweet_data\n",
    "    train_texts, train_labels = load_tweet_data('../data/tweet_sentiment.train.jsonl')\n",
    "    X_train = vectorizer.tweets_to_vectors(train_texts[:1000])  # Subset for speed\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    model = SVC(kernel='rbf', class_weight='balanced', probability=True, random_state=42)\n",
    "    model.fit(X_train, train_labels[:1000])\n",
    "    \n",
    "    # Test predictions\n",
    "    for i, tweet in enumerate(test_tweets, 1):\n",
    "        vector = vectorizer.tweet_to_vector(tweet).reshape(1, -1)\n",
    "        vector = scaler.transform(vector)\n",
    "        prediction = model.predict(vector)[0]\n",
    "        confidence = max(model.predict_proba(vector)[0])\n",
    "        \n",
    "        emoji = {\"positive\": \"üòä\", \"negative\": \"üò†\", \"neutral\": \"üòê\"}[prediction]\n",
    "        print(f\"{i}. {emoji} {prediction.upper()} ({confidence:.3f})\")\n",
    "        print(f\"   {tweet}\\n\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Demo requires data files: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Project Summary\n",
    "\n",
    "### ‚úÖ Assignment Completed\n",
    "- **GloVe embeddings**: ‚úÖ Implemented with caching\n",
    "- **CPU-friendly models**: ‚úÖ SVM, Logistic Regression, etc.\n",
    "- **Confusion matrix**: ‚úÖ Generated with analysis  \n",
    "- **Error analysis**: ‚úÖ Detailed misclassification study\n",
    "- **Reflection**: ‚úÖ Complete methodology review\n",
    "\n",
    "### üèÜ Final Results\n",
    "- **Performance**: 74.38% weighted F1-score\n",
    "- **Best Model**: SVM with RBF kernel\n",
    "- **Methodology**: Systematic experimentation\n",
    "- **Status**: Ready for production deployment\n",
    "\n",
    "### üìÅ Deliverables\n",
    "- `final_evaluation/` - Confusion matrices and performance metrics\n",
    "- `experiments/results/` - Complete experimental comparison\n",
    "- `reflection.md` - Methodology analysis\n",
    "- `ASSIGNMENT_SUMMARY.md` - Complete project overview"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv", 
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python", 
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}